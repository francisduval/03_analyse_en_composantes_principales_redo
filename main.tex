\documentclass[8pt]{beamer}
\usetheme{MonMetropolis}


% Packages ----------------------------------------------------------------------------------------------------------------------


\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage[scale = 2]{ccicons}
\usepackage{pgfplots}
\usepackage{xspace}
\usepackage{tikz, pgf}
\usepackage{bm}
\usepackage{flowchart}
\usepackage{mathtools}
\usepackage[font = {small, it}]{caption}
\usepackage[cyr]{aeguill}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage[mathscr]{eucal}
\usepackage{eurosym}
\usepackage{subfigure}
\usepackage{colortbl, color}
\usepackage{icomma}
\usepackage[numbers]{natbib}
\usepackage{mathtools}
\usepackage{numprint}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{fourier} 
\usepackage{makecell}
\usepackage{tabularx, ragged2e}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[frenchb]{babel}
\usepackage{tcolorbox}
\usepackage{lipsum}
\usepackage{tabu}
\usepackage{array}
\usepackage{smartdiagram}
% \usepackage{algorithmic}
\usepackage{bbm}
\usepackage{xfrac}
\usepackage{adjustbox}
% \usepackage{algorithm}
% \usepackage{algorithm2e}
\usepackage{pifont}
\usepackage{bbold}
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage[autostyle]{csquotes}
\usepackage[absolute, overlay]{textpos}
\usepackage[colorlinks = true, linkcolor = bluecite, urlcolor = bluecite]{hyperref}
\usepackage{textcomp}

\usetikzlibrary{calc}
\setbeamertemplate{footline}[frame number]


\usepackage{framed,color,verbatim}
\colorlet{shadecolor}{mgris!7}

\newenvironment{code}%
   {\snugshade\verbatim}%
   {\endverbatim\endsnugshade}



\definecolor{bluecite}{HTML}{0875b7}
\hypersetup{citecolor = bluecite}



% Autres commandes --------------------------------------------------------------------------------------------------------------

\usepgfplotslibrary{dateplot}
\usesmartdiagramlibrary{additions}
\usetikzlibrary{shapes, arrows, chains, arrows.meta, positioning, quotes, matrix, snakes, trees, shadows, calc, shapes.geometric, shapes.misc}
\newcommand\blt{\item[$\bullet$]}
\newcommand\fleche{\item[$\blacktriangleright$]}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\newcommand{\defeq}{\vcentcolon=}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}
\newcommand{\emphcol}[1]{\textcolor{morange}{#1}}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newcolumntype{?}{!{\vrule width 1pt}}
\DeclareMathOperator*{\moyenne}{average}
\metroset{sectionpage = none}

\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
\newcommand * \circled[1]{\tikz[baseline = (char.base)]{\node[shape = circle, draw, inner sep = 2pt, thick, fill = mgris, color = mgris, text = white, font = \bfseries, scale = 0.8] (char) {#1};}}

\definecolor{light_red}{HTML}{f6a4a4}
\newcommand{\motcle}[1]{\bm{\textcolor{mon_rouge}{{#1}}}}
\newenvironment{variableblock}[3]{%
  \setbeamercolor{block body}{#2}
  \setbeamercolor{block title}{#3}
  \begin{block}{#1}}{\end{block}}
  
\newcommand{\motclef}[1]{\textcolor{mon_rouge}{{#1}}}
  
 
\AtBeginSection[]{
  \begin{frame}
  \centering	
  \Huge 
  \textcolor{mbleufonce}{\insertsection}
  \end{frame}
}

\newcounter{exemple}
\newenvironment{exemple}[1][]{
    \refstepcounter{exemple}
    \begin{exampleblock}{Exemple~\theexemple}}
    {\end{exampleblock}}

\newcounter{propo}
\newenvironment{propo}[1][]{
    \refstepcounter{propo}
    \begin{block}{Proposition~\thepropo}}
    {\end{block}}

  
% Page-titre --------------------------------------------------------------------------------------------------------------------

\title{\huge Analyse en composantes principales\\{\normalfont\Large Analyse de données en actuariat (ACT-6100)}}
\date{\large Automne 2021}  
\author{\Large Francis Duval}
\institute{\normalsize Université du Québec à Montréal}
\titlegraphic{\hspace{-0.45cm}\includegraphics[width = 3.2cm]{logoUQAM.pdf}}


% Document principal ------------------------------------------------------------------------------------------------------------

\begin{document}
\maketitle

\begin{frame}{Contenu}
 	\setbeamertemplate{section in toc}[sections numbered]
 	\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=2em\rlap{\hskip-2em$\quad$\inserttocsectionnumber.\inserttocsubsectionnumber}$\quad$\inserttocsubsection\par}
  	\tableofcontents[]
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Introduction}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Lapin de Standford}

\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_bas.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_cote.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_derriere.png}
    \end{subfigure}

    \medskip

    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_devant.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_diagonale.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[scale = 0.248]{lapin_haut.png}
    \end{subfigure}
\end{figure}

\textbf{Quelle est la meilleure projection?}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Lapin de Stanford}
    
\textbf{Projection en 2 dimensions du jeu de données grâce à l'ACP}


\begin{code}
> library(tidyverse)
> library(onion)
> theme_set(theme_bw())
> 
> acp <- prcomp(bunny)
> ggplot(as_tibble(acp[["x"]]), aes(x = PC1, y = PC2)) +
+   geom_point(size = 0.1)
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{lapin_standford_acp.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{ACP en 2 dimensions}

\begin{figure}
    \centering
    \includegraphics[scale = 0.35]{pca_2d_2.png}
\end{figure}

\begin{itemize}
    \fleche L'ACP va d'abord trouver le premier axe principal (PC1), c'est-à-dire la droite passant par $(0, 0)$ qui maximise la variance des points projetés orthogonalement sur celle-ci.
    \fleche Ensuite, l'ACP va trouver le deuxième axe principal (PC2), c'est-à-dire la droite passant par $(0, 0)$ qui maximise la variance des points projetés sur celle-ci \motcle{parmi tous les axes orthogonaux à l'axe PC1}.
    \begin{itemize}
        \item[-] Noter qu'ici, puisqu'on est en 2D, il n'y a qu'un seul axe orthogonal à la droite PC1 passant par $(0, 0)$. 
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Données}
    
\small
\begin{itemize}
    \fleche Le but est d'explorer un jeu de données \motcle{numérique}.
    \fleche Exemple: extrait de la base de données \href{https://statsandr.com/blog/data/Eurojobs.csv}{\textcolor{blue}{\underline{\texttt{eurojob}}}}, qui décrit les composantes de l'économie pour certains pays
    \begin{code}
> library(tidyverse)
> eurojob <- read_csv("https://statsandr.com/blog/data/Eurojobs.csv")
> x <- 
+   eurojob[1:6, 1:4] %>%
+   column_to_rownames("Country") %>% 
+   as.matrix()

> x
            Agr Min  Man
Belgium     3.3 0.9 27.6
Denmark     9.2 0.1 21.8
France     10.8 0.8 27.5
W. Germany  6.7 1.3 35.8
Ireland    23.2 1.0 20.7
Italy      15.9 0.6 27.6
    \end{code}
    \fleche Les lignes décrivent les \motcle{observations ou les individus} (les 6 pays) alors que les colonnes décrivent les \motcle{variables} (les 3 secteurs de l'économie: agriculture, minier et manufacturier).
    \fleche On veut savoir:
    \begin{itemize}
        \item[-] quelles observations sont similaires,
        \item[-] quelles variables sont liées.
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------


\begin{frame}[fragile]{Matrices de distances et de corrélations}

On peut observer:

\begin{itemize}
    \fleche La matrice des \motcle{distances} entre individus:
    \begin{code}
> dist(x)
             Belgium   Denmark    France W. Germany   Ireland
Denmark     8.312039                                         
France      7.501333  5.961543                               
W. Germany  8.885944 14.272001  9.270922                     
Ireland    21.062526 14.071958 14.143550  22.368505          
Italy      12.603571  8.875810  5.104900  12.343824 10.052860
    \end{code}
    \fleche La matrice des \motcle{corrélations} entre variables:
    \begin{code}
> cor(x)
            Agr         Min        Man
Agr  1.00000000 -0.01026054 -0.5573922
Min -0.01026054  1.00000000  0.6117080
Man -0.55739225  0.61170803  1.0000000
    \end{code}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Visualisation des distances et corrélations}

\begin{itemize}
    \fleche L'ACP permet de visualiser sur un graphique les \motcle{distances entre individus} ainsi que les \motcle{corrélations entre variables}.
\end{itemize}


\begin{columns}[T]
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \centering
            \includegraphics[scale = 0.27]{distances_observations.png}
        \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \centering
            \includegraphics[scale = 0.27]{correlations_variables.png}
        \end{figure}
    \end{column}
\end{columns}


\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{But de l'ACP}
\small
\begin{itemize}
    \fleche Construire de nouvelles variables numériques qui résument le mieux possible les variables originales afin de \motcle{réduire la dimension} du jeu de données.
    \begin{code}
> library(FactoMineR)

> # Jeu de données original
> x
            Agr Min  Man
Belgium     3.3 0.9 27.6
Denmark     9.2 0.1 21.8
France     10.8 0.8 27.5
W. Germany  6.7 1.3 35.8
Ireland    23.2 1.0 20.7
Italy      15.9 0.6 27.6

> # Nouvelles variables synthétiques
> acp <- PCA(x)
> acp$ind$coord
                Dim.1       Dim.2       Dim.3
Belgium     0.8804620 -0.72493205 -0.65397292
Denmark    -1.5141449 -1.50391377  0.06360123
France      0.1719596 -0.05132316  0.02039562
W. Germany  2.3694452  0.38961453  0.21632994
Ireland    -1.4359290  1.72305846 -0.33486793
Italy      -0.4717928  0.16749598  0.68851406
    \end{code}
\end{itemize}

\end{frame}


% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{En résumé}
    
\small
\begin{itemize}
    \fleche L'analyse en composantes principales (ACP) est une méthode de \motcle{réduction de dimensions}.
    \begin{itemize}
        \item[-] Dans un jeu de données, le nombre de dimensions correspond au nombre de \motcle{variables}.
        \item[-] L'ACP permet donc de réduire le nombre de variables dans un jeu de données tout en \motcle{préservant un maximum d'information}.
        \item[-] On souhaite décrire la variablilité (l'inertie) présente dans un ensemble de variables initiales corrélées
        \begin{align*}
            X_1, \dots, X_p
        \end{align*}
        à l'aide d'un nouvel ensemble de variables non-corrélées
        \begin{align*}
            PC_1, \dots, PC_q,
        \end{align*}
        appelées \motcle{composantes principales}.
        \item[-] Chacune des nouvelles variables $PC_j, j = 1, \dots, q$, est une combinaison linéaire des variables initiales $X_j, j = 1, \dots, q$:
        \begin{align*}
            PC_j = a_{j1} X_1 + a_{j2} X_2 + \dots + a_{jk} X_p.
        \end{align*}
        \item[-] Pour réduire la dimensionnalité, l'ACP \motcle{projette} (de manière optimale) le nuage de points des observations dans $\mathbb{R}^p$ dans l'espace $\mathbb{R}^{q}$, où $q < p$.
        \item[-] « Préserver un maximum d'information » signifie qu'on veut que 2 points qui sont \motcle{proches dans l'espace $\mathbb{R}^p$} soient également \motcle{proches dans l'espace $\mathbb{R}^q$}. 
        \item[-] De la même manière, on veut que 2 points qui sont \motcle{éloignés dans l'espace $\mathbb{R}^p$} soient également \motcle{éloignés dans l'espace $\mathbb{R}^q$}.
    \end{itemize}
    \fleche C'est par le fait même un outil de \motcle{visualisation des données} puisqu'elle permet de visualiser des jeux de données en plus de 3 dimensions.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Concepts de base}

% ------------------------------------------------------------------------------------------------------------------------------

\subsection{Nuage des observations}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Notation}
    \begin{itemize}
        \fleche On considère une base de données numérique où $n$ observations sont décrites avec $p$ variables.
        \begin{table}
            \centering
            \begin{tabular}{c|ccccc|}
                & 1 & \cdots & $j$ & \cdots & $p$\\
                \midrule
                1 &  &  &  &  & \\
                \vdots &  &  & \vdots &  & \\
                $i$ &  & \cdots & $x_{ij}$ & \cdots & \\
                \vdots &  &  & \vdots &  & \\
                $n$ &  &  &  &  & \\
                \midrule
            \end{tabular}
        \end{table}
        \fleche Un peu de notation:
        \begin{itemize}
            \blt $\boldsymbol{X} = (x_{ij})_{n\times p}$ est la matrice des données, où $x_{ij}$ est la valeur de la $i^\text{e}$ observation pour la $j^\text{e}$ variable.
            \blt La $i^\text{e}$ ligne et la $j^\text{e}$ colonne de $\boldsymbol{X}$ sont notées respectivement
            \begin{align*}
                \boldsymbol{x}_i = \begin{pmatrix}x_{i1}\\\vdots \\ x_{ip}\end{pmatrix}\in \mathbb{R}^p \quad \text{et} \quad \boldsymbol{x}^j = \begin{pmatrix}x_{1j}\\\vdots \\ x_{nj}\end{pmatrix}\in \mathbb{R}^n.
            \end{align*}
        \end{itemize}

    \end{itemize}
\end{frame}


% -------------------------------------------------------------------------------------------------------------------
%-----------

\begin{frame}{Nuage des observations}
\small
\textbf{Exemple}: les 6 pays d'\texttt{eurojob} définissent un nuage de points de $n = 6$ points dans $\mathbb{R}^3$.
    
\begin{figure}
    \centering
    \includegraphics[scale = 0.37]{nuage_observations.png}
\end{figure}
    
\begin{itemize}
    \fleche Les $n$ lignes de $\boldsymbol{X}$, $\boldsymbol{x}_i$ pour $i = 1, \dots n$, définissent un nuage de points dans $\mathbb{R}^p$.
    \fleche Chaque observation $i$ est un \motcle{point $\boldsymbol{x}_i$ dans $\mathbb{R}^p$}.
    \fleche Un \motclef{poids $w_i$} est associé à chaque observation. Habituellement:
    \begin{itemize}
        \item[-] $w_i = \frac{1}{n}$ pour des observations tirées au hasard
        \item[-] $w_i \ne \frac{1}{n}$ pour des observations aggrégées, etc.
    \end{itemize}
    \fleche Pour réaliser une ACP, on a besoin du \motcle{nuage centré} ou du \motcle{nuage centré-réduit}.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Nuage centré des observations}

\small
\begin{columns}[T]
    \begin{column}{0.48\textwidth}
        Matrice des données \motcle{originale} $\boldsymbol{X}$
        \begin{table}
            \centering
            \begin{tabular}{c|ccccc|}
                & 1 & \cdots & $j$ & \cdots & $p$\\
                \midrule
                1 &  &  &  &  & \\
                \vdots &  &  & \vdots &  & \\
                $i$ &  & \cdots & $x_{ij}$ & \cdots & \\
                \vdots &  &  & \vdots &  & \\
                $n$ &  &  &  &  & \\
                \midrule
                \midrule
                $\boldsymbol{\overline{x}}$ &  & \cdots & $\overline{x}^j$ & \cdots &
            \end{tabular}
        \end{table}
    \end{column}
    \begin{column}{0.48\textwidth}
        Matrice des données \motcle{centrée} $\boldsymbol{Y}$
        \begin{table}
            \centering
            \begin{tabular}{c|ccccc|}
                & 1 & \cdots & $j$ & \cdots & $p$\\
                \midrule
                1 &  &  &  &  & \\
                \vdots &  &  & \vdots &  & \\
                $i$ &  & \cdots & $y_{ij}$ & \cdots & \\
                \vdots &  &  & \vdots &  & \\
                $n$ &  &  &  &  & \\
                \midrule
                \midrule
                $\boldsymbol{\overline{y}}$ &  & \cdots & $0$ & \cdots &
            \end{tabular}
        \end{table}
    \end{column}
\end{columns}

Ici:
\begin{itemize}
    \fleche $\overline{x}^j$ est la moyenne empirique de la $j^\text{e}$ variable,
    \fleche \motclef{$y_{ij} = x_{ij} - \overline{x}^j$} est le terme générique de la matrice centrée des données $\boldsymbol{Y}$.
    \fleche Les colonnes de la \motcle{matrice centrée $\boldsymbol{Y}$} ont une moyenne de 0:
    \begin{align*}
        \boldsymbol{\overline{y}} = \frac{1}{n}\sum_{i = 1}^n y_{ij} = 0.
    \end{align*}
    \fleche Le nouveau centre de gravité du nuage est le point d’origine.
    \fleche Les distances entre les observations sont préservées, c'est-à-dire que
    \begin{align*}
        d_{\boldsymbol{M}}(\boldsymbol{x}_i, \boldsymbol{x}_j) = d_{\boldsymbol{M}}(\boldsymbol{y}_i, \boldsymbol{y}_j),
    \end{align*}
    où $d_{\boldsymbol{M}}(\cdot, \cdot)$ est une mesure de distance.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Nuage centré des observations}

\small
\textbf{Exemple:} jeu de données \texttt{eurojob}

\begin{code}
> # Centrer la matrice des données x
> y <- x
> for(j in 1:ncol(x)) {
+     y[ , j] <- x[ , j] - mean(x[ , j])
+ }
\end{code}

\begin{columns}[T]
    \begin{column}{0.45\textwidth}
        Matrice des données \motcle{originale} $\boldsymbol{X}$
        \begin{code}
> round(x, 4)
            Agr Min  Man
Belgium     3.3 0.9 27.6
Denmark     9.2 0.1 21.8
France     10.8 0.8 27.5
W. Germany  6.7 1.3 35.8
Ireland    23.2 1.0 20.7
Italy      15.9 0.6 27.6
        \end{code}
        Moyennes des colonnes de $\boldsymbol{X}$
        \begin{code}
> round(colMeans(x), 4)
    Agr     Min     Man 
11.5167  0.7833 26.8333
        \end{code}
    \end{column}
    \begin{column}{0.45\textwidth}
        Matrice des données \motcle{centrée} $\boldsymbol{Y}$
        \begin{code}
> round(y, 4)
               Agr     Min     Man
Belgium    -8.2167  0.1167  0.7667
Denmark    -2.3167 -0.6833 -5.0333
France     -0.7167  0.0167  0.6667
W. Germany -4.8167  0.5167  8.9667
Ireland    11.6833  0.2167 -6.1333
Italy       4.3833 -0.1833  0.7667
        \end{code}
         Moyennes des colonnes de $\boldsymbol{Y}$
        \begin{code}
> round(colMeans(y), 4)
Agr Min Man 
  0   0   0 
        \end{code}
    \end{column}
\end{columns}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Nuage centré des observations}

Centrer les données s'interprète comme une translation du nuage des obseravtions dans $\mathbb{R}^p$.
    \begin{figure}
    \centering
    \includegraphics[scale = 0.52]{nuage_centre.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Importance de centrer les variables en ACP}
    
\begin{figure}
    \centering
    \includegraphics[scale = 0.45]{importance_centrer_nuage.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Nuage normalisé (centré-réduit) des observations}

\small
\begin{columns}[T]
    \begin{column}{0.48\textwidth}
        Matrice des données \motcle{originale} $\boldsymbol{X}$
        \begin{table}
            \centering
            \begin{tabular}{c|ccccc|}
                & 1 & \cdots & $j$ & \cdots & $p$\\
                \midrule
                1 &  &  &  &  & \\
                \vdots &  &  & \vdots &  & \\
                $i$ &  & \cdots & $x_{ij}$ & \cdots & \\
                \vdots &  &  & \vdots &  & \\
                $n$ &  &  &  &  & \\
                \midrule
                \midrule
                $\boldsymbol{\overline{x}}$ &  & \cdots & $\overline{x}^j$ & \cdots &\\
                $\boldsymbol{s}$ &  & \cdots & $s_j$ & \cdots &
            \end{tabular}
        \end{table}
    \end{column}
    \begin{column}{0.48\textwidth}
        Matrice des données \motcle{centrée-réduite} $\boldsymbol{Z}$
        \begin{table}
            \centering
            \begin{tabular}{c|ccccc|}
                & 1 & \cdots & $j$ & \cdots & $p$\\
                \midrule
                1 &  &  &  &  & \\
                \vdots &  &  & \vdots &  & \\
                $i$ &  & \cdots & $z_{ij}$ & \cdots & \\
                \vdots &  &  & \vdots &  & \\
                $n$ &  &  &  &  & \\
                \midrule
                \midrule
                $\boldsymbol{\overline{z}}$ &  & \cdots & $0$ & \cdots &\\
                $\boldsymbol{s}$ &  & \cdots & $1$ & \cdots &
            \end{tabular}
        \end{table}
    \end{column}
\end{columns}

Ici:
\begin{itemize}
    \fleche $s_j^2 = \frac{1}{n} \sum_{i = 1}^n (x_{ij} - \overline{x}^j)^2$ est la variance empirique de la $j^\text{e}$ variable,
    \fleche \motclef{$z_{ij} = \frac{x_{ij} - \overline{x}^j}{s_j}$} est le terme générique de la matrice centrée-réduite des données $\boldsymbol{Z}$.
    \fleche Les colonnes de la \motcle{matrice centrée-réduite $\boldsymbol{Z}$} ont une moyenne de 0 et une variance de 1:
    \begin{align*}
        \overline{z}^j = \frac{1}{n}\sum_{i = 1}^n z_{ij} = 0, \quad \text{Var}(\boldsymbol{z}^j) = \frac{1}{n}\sum_{i = 1}^n (z_{ij} - \overline{z}^j)^2 = \frac{1}{n} \sum_{i = 1}^n z_{ij}^2 = 1.
    \end{align*}
    \fleche Les distances entre les observations ne sont pas préservées, c'est-à-dire que
    \begin{align*}
        d_{\boldsymbol{M}}(\boldsymbol{x}_i, \boldsymbol{x}_j) \ne d_{\boldsymbol{M}}(\boldsymbol{z}_i, \boldsymbol{z}_j),
    \end{align*}
    où $d_{\boldsymbol{M}}(\cdot, \cdot)$ est une mesure de distance.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Nuage centré-réduit des observations}

\small
\textbf{Exemple:} jeu de données \texttt{eurojob}

\begin{code}
> # Centrer-réduire la matrice des données x
> ecart_type <- function(x) {sqrt(mean((x - mean(x))^2))}
> z <- x
> for(j in 1:ncol(x)) {
+     z[ , j] <- (x[ , j] - mean(x[ , j])) / ecart_type(x[ , j])
+ }
\end{code}

\begin{columns}[T]
    \begin{column}{0.45\textwidth}
        Matrice des données \motcle{originale} $\boldsymbol{X}$
        \begin{code}
> round(x, 4)
            Agr Min  Man
Belgium     3.3 0.9 27.6
Denmark     9.2 0.1 21.8
France     10.8 0.8 27.5
W. Germany  6.7 1.3 35.8
Ireland    23.2 1.0 20.7
Italy      15.9 0.6 27.6
        \end{code}
        Écart-type des colonnes de $\boldsymbol{X}$
        \begin{code}
> round(apply(x, 2, ecart_type), 4)
   Agr    Min    Man 
6.4847 0.3716 4.9155 
        \end{code}
    \end{column}
    \begin{column}{0.45\textwidth}
        Matrice des données \motcle{centrée-réduite} $\boldsymbol{Z}$
        \begin{code}
> round(z, 4)
               Agr     Min     Man
Belgium    -1.2671  0.3140  0.1560
Denmark    -0.3573 -1.8391 -1.0240
France     -0.1105  0.0449  0.1356
W. Germany -0.7428  1.3905  1.8242
Ireland     1.8017  0.5831 -1.2478
Italy       0.6759 -0.4934  0.1560
        \end{code}
        Écart-type des colonnes de $\boldsymbol{Z}$
        \begin{code}
> round(apply(z, 2, ecart_type), 4)
Agr Min Man 
  1   1   1
        \end{code}
    \end{column}
\end{columns}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Nuage centré-réduit des observations}

Normaliser (centrer-réduire) les données s'interprète comme une translation suivie d'une normalisation du nuage des obseravtions dans $\mathbb{R}^p$.
    \begin{figure}
    \centering
    \includegraphics[scale = 0.52]{nuage_centre_reduit.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Distance entre 2 observations}
    
\begin{itemize}
    \fleche La proximité entre 2 observations peut être mesurée avec la \motcle{distance euclidienne}.
    \fleche La distance euclidienne entre 2 observations $i$ et $i'$ (2 lignes de $\boldsymbol{X}$) est
    \begin{align*}
        d_2(\boldsymbol{x}_i, \boldsymbol{x}_{i'}) = \sum_{i = 1}^p (x_{ij} - x_{i'j})^2.
    \end{align*}
    \fleche Quand les données sont \motcle{centrées-réduites}, la \motcle{distance euclidienne} entre 2 observations $\boldsymbol{z}_i$ et $\boldsymbol{z}_{i'}$ (2 lignes de $\boldsymbol{Z}$) est 
    \begin{align*}
        d_2(\boldsymbol{z}_i, \boldsymbol{z}_{i'}) = \sum_{i = 1}^p \frac{1}{s_j^2}(x_{ij} - x_{i'j})^2 = \sum_{i = 1}^p (z_{ij} - z_{i'j})^2.
    \end{align*}
\end{itemize}

\textbf{Cela signifie que:}

\begin{itemize}
    \fleche Si les variables (colonnes de $\boldsymbol{X}$) sont mesurées avec des \motcle{échelles différentes}, les variables avec des variances plus grandes seront plus importantes lors du calcul de la distance euclidienne.
    \fleche Normaliser les données permet de donner la \motcle{même importance} à toutes les variables lors du calcul de la distance.
    \fleche La plupart du temps, on utilise donc la \motcle{distance euclidienne} (métrique identité $\boldsymbol{M} = \boldsymbol{I}$) sur les \motcle{données centrées-réduites}.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Inertie}

\small
\begin{itemize}
    \fleche L'\motcle{inertie} d'un nuage de points (ou d'observations) est une mesure de dispersion de ce même nuage. Elle est définie par 
    \begin{align*}
        \mathbb{I}(\boldsymbol{X}) = \sum_{i = 1}^n w_i d_{\boldsymbol{M}}^2(\boldsymbol{x}_i, \overline{\boldsymbol{x}}),
    \end{align*}
    où $\overline{\boldsymbol{x}} = (\overline{x}^1, \dots, \overline{x}^p)$ est le centre de gravité du nuage.
\end{itemize}

\begin{propo}
    \begin{itemize}
        \fleche Si on choisit une métrique diagonale $\boldsymbol{M} = \text{diag}(m_1, \dots, m_k)$ et $w_i = 1/n\quad \forall i$, on obtient 
        \begin{align*}
            \mathbb{I}(\boldsymbol{X}) = \sum_{j = 1}^p m_j \text{Var}(\boldsymbol{x}^j)
        \end{align*}
    \fleche En particulier, si on utilise $\boldsymbol{M} = \boldsymbol{I}$, alors 
    \begin{align*}
        \mathbb{I}(\boldsymbol{X}) = \sum_{j = 1}^p \text{Var}(\boldsymbol{x}^j) = \sum_{j = 1}^p s_j^2
    \end{align*}
    et
    \begin{align*}
         \mathbb{I}(\boldsymbol{Z}) = p.
    \end{align*}
    \end{itemize}
    *Faire la démonstration en exercice.
\end{propo}

\begin{itemize}
    \fleche On peut donc voir l'inertie comme une \motcle{variance généralisée} à plusieurs dimensions.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Inertie}

\small
\textbf{Exemple:} inertie du jeu de données \texttt{eurojob} (en supposant $w_i = 1/n \quad \forall i$ et $\boldsymbol{M} = \boldsymbol{I}$).
\vspace{0.3cm}
\begin{columns}
    \begin{column}{0.48\textwidth}
       Données \motcle{centrées} $\boldsymbol{Y}$
        \begin{code}
> round(y, 2)
             Agr   Min   Man
Belgium    -8.22  0.12  0.77
Denmark    -2.32 -0.68 -5.03
France     -0.72  0.02  0.67
W. Germany -4.82  0.52  8.97
Ireland    11.68  0.22 -6.13
Italy       4.38 -0.18  0.77
        \end{code}
        Variances des colonnes
        \begin{code}
> round(apply(x, 2, ecart_type) ^ 2, 4)
    Agr     Min     Man 
42.0514  0.1381 24.1622 
        \end{code}
    \end{column}
    \begin{column}{0.48\textwidth}
        Données \motcle{centrées-réduites} $\boldsymbol{Z}$
        \begin{code}
> round(z, 2)
             Agr   Min   Man
Belgium    -1.27  0.31  0.16
Denmark    -0.36 -1.84 -1.02
France     -0.11  0.04  0.14
W. Germany -0.74  1.39  1.82
Ireland     1.80  0.58 -1.25
Italy       0.68 -0.49  0.16
        \end{code}
        Variance des colonnes
        \begin{code}
> round(apply(z, 2, ecart_type) ^ 2, 4)
Agr Min Man 
  1   1   1 
        \end{code}
    \end{column}
\end{columns}

\begin{itemize}
    \fleche Inertie du jeu de données \motcle{centré}:
    \begin{align*}
        I(\boldsymbol{Y}) = 42.0514 + 0.1381 + 24.1622 = 70.3517.
    \end{align*}
    \fleche Inertie du jeu de données \motcle{centré-réduit}:
    \begin{align*}
        I(\boldsymbol{Z}) = 1 + 1 + 1 = 3.
    \end{align*}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\subsection{Nuage des variables}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Nuage des variables}

\textbf{Exemple:} les 3 industries d'\texttt{eurojob} (Agr, Min et Man) définissent un nuage de $p = 3$ points dans $\mathbb{R}^6$. On ne peut pas visualiser ce nuage de points, car il est en 6 dimensions.
\small
\begin{code}
> t(x)
    Belgium Denmark France W. Germany Ireland Italy
Agr     3.3     9.2   10.8        6.7    23.2  15.9
Min     0.9     0.1    0.8        1.3     1.0   0.6
Man    27.6    21.8   27.5       35.8    20.7  27.6
\end{code}
\normalsize
\begin{itemize}
    \fleche Chaque variable $j$ est un point $\boldsymbol{x}^j$ dans $\mathbb{R}^n$ (une colonne de $\boldsymbol{X}$).
    \fleche Un poids $m_j$ est associé à chaque variable $j$.
    \begin{itemize}
        \item[-] $m_j = 1$ en ACP.
        \item[-] $m_j \ne 1$ en analyse des correspondances multiples.
    \end{itemize}
\end{itemize}


\begin{itemize}
    \fleche Quand les données sont centrées:
    \begin{itemize}
        \item[-] chaque variable $j$ est un point noté $\boldsymbol{y}^j$ dans $\mathbb{R}^n$,
        \item[-] on parle des variables centrées.
    \end{itemize}
    \fleche Quand les données sont centrées-réduites:
    \begin{itemize}
        \item[-] chaque variable $j$ est un point noté $\boldsymbol{z}^j$ dans $\mathbb{R}^n$,
        \item[-] on parle des variables normalisées.
    \end{itemize}
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Lien entre 2 variables}

Il n'est pas naturel de parler de distance entre 2 variables. On mesure plutôt le lien entre 2 variables en utilisant la \motcle{covariance} ou la \motcle{corrélation} empiriques.

\begin{itemize}
    \fleche Pour définir la covariance et la corrélation, un métrique $\boldsymbol{N}$ est introduite dans l'espace $\mathbb{R}^n$:
    \begin{align*}
        \boldsymbol{N} = \text{diag}(1/n, \dots, 1/n).
    \end{align*}
    \begin{itemize}
        \item[-] Cette métrique est la plus naturelle à utiliser pour cet espace puisqu'elle nous permet d'exprimer certaines quantités (écart-type, variance, covariance, corrélation, etc.) en termes de produits scalaires.
    \end{itemize}
    \fleche Le produit scalaire entre $\boldsymbol{x}$ et $\boldsymbol{y}$ dans $\mathbb{R}^n$ est défini par 
    \begin{align*}
        \langle \boldsymbol{x}, \boldsymbol{y} \rangle_{\boldsymbol{N}} = \boldsymbol{x}^\top \boldsymbol{N} \boldsymbol{y} = \frac{1}{n} \boldsymbol{x}^\top \boldsymbol{y} = \frac{1}{n}\sum_{i = 1}^n x_i y_i.
    \end{align*}
    \fleche La norme de $\boldsymbol{x}$ dans $\mathbb{R}^n$ est alors
    \begin{align*}
        ||\boldsymbol{x}||_{\boldsymbol{N}} = \sqrt{\langle \boldsymbol{x}, \boldsymbol{x} \rangle_{\boldsymbol{N}}} = \sqrt{\frac{1}{n} \sum_{i = 1}^n x_i^2}.
    \end{align*}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Variance, covariance et corrélation}

\begin{itemize}
    \fleche Avec cette métrique, la \motcle{variance} peut s'écrire comme une \motcle{norme au carré}:
    \begin{itemize}
        \blt $\text{Var}(\boldsymbol{x}^j) = \frac{1}{n}\sum_{i = 1}^n (x_{ij} - \overline{x}^j)^2 = ||\boldsymbol{y}^j||_{\boldsymbol{N}}^2$,
        \blt $\text{Var}(\boldsymbol{z}^j) = \frac{1}{n}\sum_{i = 1}^n (z_{ij} - \overline{z}^j)^2 = ||\boldsymbol{z}^j||_{\boldsymbol{N}}^2$ = 1.
    \end{itemize}
    \fleche Les $p$ variables sont donc situées sur la sphère unitaire de $\mathbb{R}^n$ (puisque $||\boldsymbol{z}^j||_{\boldsymbol{N}} = 1$).
    \fleche De plus, la \motcle{covariance} et la \motcle{corrélation} s'expriment comme un produit scalaire:
    \begin{itemize}
        \blt $c_{j, j'} = \frac{1}{n} \sum_{i = 1}^n(x_{ij} - \overline{x}^j)(x_{ij'} - \overline{x}^{j'}) = \langle \boldsymbol{y}^j, \boldsymbol{y}^{j'}\rangle_{\boldsymbol{N}}$,
        \blt $r_{j, j'} = \frac{1}{n} \sum_{i = 1}^n\left(\frac{x_{ij} - \overline{x}^j}{s_j}\right)\left(\frac{x_{ij'} - \overline{x}^{j'}}{s_{j'}}\right) = \langle \boldsymbol{z}^j, \boldsymbol{z}^{j'}\rangle_{\boldsymbol{N}}$.
    \end{itemize}
    \fleche Ceci conduit à une expression simple de la matrice de covariance notée $\boldsymbol{C}$ et de la matrice de corrélation notée $\boldsymbol{R}$:
    \begin{itemize}
        \blt $\boldsymbol{C} = \boldsymbol{Y}^\top \boldsymbol{N}\boldsymbol{Y}$,
        \blt $\boldsymbol{R} = \boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{Z}$.
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Interprétation géométrique de la corrélation entre 2 variables}

\begin{itemize}
    \fleche Avec la métrique $\boldsymbol{N}$, la \motcle{corrélation} entre 2 variables s'écrit comme le \motcle{cosinus de l'angle} entre celles-ci:
    \begin{itemize}
        \blt $r_{jj'} = \frac{\langle \boldsymbol{y}^j, \boldsymbol{y}^{j'}\rangle_{\boldsymbol{N}}}{||\boldsymbol{y}^j||_{\boldsymbol{N}} ||\boldsymbol{y}^{j'}||_{\boldsymbol{N}}} = \cos[\theta(\boldsymbol{y}^j, \boldsymbol{y}^{j'})]$,
        \blt $r_{jj'} = \langle \boldsymbol{z}^j, \boldsymbol{z}^{j'}\rangle_{\boldsymbol{N}} = \cos[\theta(\boldsymbol{z}^j, \boldsymbol{z}^{j'})]$.
    \end{itemize}
    \fleche Ceci nous permet d'avoir une \motcle{interprétation géométrique de la corrélation} entre 2 variables:
     \begin{itemize}
        \item[-] un angle droit (90 degrés ou $\pi/2$ radians) correspond à une corrélation nulle;
        \item[-] un angle nul correspond à une corrélation de 1; et
        \item[-] un angle plat (180 degrés ou $\pi$ radians) correspond à une corrélation de -1.
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{En bref}
    
\begin{itemize}
    \fleche Une ACP peut se faire à partir
    \begin{itemize}
        \item[-] des données centrées $\boldsymbol{Y}$; ou 
        \item[-] des données centrées-réduites $\boldsymbol{Z}$.
    \end{itemize}
    \fleche Ceci conduit à 2 types d'ACP:
    \begin{itemize}
        \item[-] l'ACP non standardisée (ou ACP sur la matrice de covariance), qui analyse $\boldsymbol{Y}$,
        \item[-] l'ACP standardisée (ou ACP sur la matrice de corrélation), qui analyse $\boldsymbol{Z}$.
    \end{itemize}
    \fleche Une ACP se fait en étudiant 2 nuages: le nuage des $n$ \motcle{observations} dans $\mathbb{R}^p$ avec la métrique $\boldsymbol{M}$ (en pratique, on prend $\boldsymbol{M} = \boldsymbol{I}_p$) et le nuage des $p$ \motcle{variables} dans $\mathbb{R}^n$ avec la métrique $\boldsymbol{N} = (1/n)\boldsymbol{I}_n$.
\end{itemize}

\textbf{À partir de maintenant, nous ne considérerons que l'ACP standardisée en utilisant la métrique $\boldsymbol{I}_p$ pour le nuage des observations et la métrique $\boldsymbol{N} = (1/n)\boldsymbol{I}_n$ pour le nuage des variables.}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Analyse du nuage des observations}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Analyse du nuage des observations}
    
\begin{exampleblock}{Objectif}
    \begin{itemize}
        \fleche Trouver un espace (ici un plan) tel que les distances entre les observations soient les mieux préservées.
    \end{itemize}
\end{exampleblock}

\begin{figure}
    \centering
    \includegraphics[scale = 0.25]{nuage_projection.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Projection d'une observation sur un axe}


\begin{itemize}
    \fleche La \motcle{coordonée de la projection} orthogonale d'un point $\boldsymbol{z}_i \in \mathbb{R}^p$ sur un axe $\boldsymbol{\Delta}_\alpha$ défini par le vecteur unitaire $\boldsymbol{v}_\alpha$ ($\boldsymbol{v}_\alpha^\top \boldsymbol{v}_\alpha = 1$) est:
    \begin{align*}
        f_{i\alpha} = \langle \boldsymbol{z}_i, \boldsymbol{v}_\alpha \rangle = \boldsymbol{z}_i^\top \boldsymbol{v}_\alpha.
    \end{align*}
     \fleche Pour s'en convaincre, se souvenir que $\langle \boldsymbol{x}, \boldsymbol{y} \rangle$ est la \motcle{norme} du \motcle{vecteur $\boldsymbol{x}$ projeté orthogonalement sur le vecteur $\boldsymbol{y}$} multipliée par la \motcle{norme de $\boldsymbol{x}$}.
    \fleche Le \motcle{vecteur des coordonées} sur l'axe $\boldsymbol{\Delta}_\alpha$ pour les $n$ observations est:
    \begin{align*}
        \boldsymbol{f}^\alpha = \begin{pmatrix} f_{1\alpha} \\ \vdots \\ f_{n\alpha} \end{pmatrix} = \boldsymbol{Z}\boldsymbol{v}_\alpha = \sum_{i = 1}^p v_{j\alpha} \boldsymbol{z}^j.
    \end{align*}
    \begin{itemize}
        \blt $\boldsymbol{f}^\alpha$ est une \motcle{combinaison linéaire} des colonnes de $\boldsymbol{Z}$.
        \blt $\boldsymbol{f}^\alpha$ est \motcle{centré} (en supposant que les colonnes de $\boldsymbol{Z}$ soient centrées).
    \end{itemize}
\end{itemize}

En ACP, les vecteurs $\boldsymbol{v}_1, \boldsymbol{v}_2, \dots$ sont choisis de manière à \motcle{maximiser l'inertie des projections} des observations.

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Projection d'une observation sur un axe -- Exemple}

\small 
\begin{exemple}
    \begin{itemize}
        \fleche On considère la matrice \texttt{eurojob} centrée-réduite
        \begin{align*}
            \boldsymbol{Z} = 
            \begin{bmatrix}
                -1.2671 & 0.3140 & 0.1560 \\ 
                -0.3573 & -1.8391 & -1.0240 \\ 
                -0.1105 & 0.0449 & 0.1356 \\ 
                -0.7428 & 1.3905 & 1.8242 \\ 
                1.8017 & 0.5831 & -1.2478 \\ 
                0.6759 & -0.4934 & 0.1560 \\ 
            \end{bmatrix}
        \end{align*}
        \fleche On veut projeter les 6 observations sur les axes orthogonaux $\boldsymbol{\Delta}_1$ et $\boldsymbol{\Delta}_2$ définis respectivement par les vecteurs unitaires
        \begin{align*}
            \boldsymbol{v}_1 = \begin{bmatrix} -0.4783158 \\ 0.5237373 \\ 0.7049207 \end{bmatrix}\quad \text{et}\quad
            \boldsymbol{v}_2 = \begin{bmatrix} 0.739170360 \\ 0.673517525 \\ 0.00114991 \end{bmatrix}.
        \end{align*}
        \fleche Calculer $\boldsymbol{f}^1$ et $\boldsymbol{f}^2$.
    \end{itemize}
\end{exemple}

\textbf{Réponse:}

$\boldsymbol{f}^1 = \boldsymbol{Z}\boldsymbol{v}_1$ et $\boldsymbol{f}^2 = \boldsymbol{Z}\boldsymbol{v}_2$.
\begin{itemize}
    \fleche $\boldsymbol{f}^1$ nous donne les coordonnées des 6 observations projetées sur l'axe définit par $\boldsymbol{v}_1$.
    \fleche $\boldsymbol{f}^2$ nous donne les coordonnées des 6 observations projetées sur l'axe définit par $\boldsymbol{v}_2$.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Sélection des vecteurs $\boldsymbol{v}$}

\begin{itemize}
    \fleche Dans un \motcle{premier temps}, on cherche un axe $\boldsymbol{\Delta}_1$ défini par un vecteur unitaire $\boldsymbol{v}_1\in\mathbb{R}^p$ tel que
    \begin{align*}
        \boldsymbol{v}_1 &= \argmax{||\boldsymbol{v}|| = 1} \text{Var}(\boldsymbol{Z}\boldsymbol{v}) \\
        &= \argmax{||\boldsymbol{v}|| = 1} \boldsymbol{v}^\top \boldsymbol{R} \boldsymbol{v},
    \end{align*}
    où \begin{align*}
        \boldsymbol{R} = \frac{1}{n}\boldsymbol{Z}^\top \boldsymbol{Z} \in \mathbb{R}^{p\times p}
    \end{align*}
     est la matrice de corrélation.
    \fleche On cherche donc l'axe $\boldsymbol{\Delta}_1$ qui \motcle{maximize la variance des observations projetées} sur ce même axe.
\end{itemize}

\begin{itemize}
    \fleche On peut montrer que:
    \begin{itemize}
        \blt $\boldsymbol{v}_1$ est le \motcle{vecteur propre} associé à la \motcle{plus grande} valeur propre $\lambda_1$ de $\boldsymbol{R}$,
        \blt La \motcle{première composante principale} (PC1) $\boldsymbol{f}^1 = \boldsymbol{Z}\boldsymbol{v}_1$ est centrée:
        \begin{align*}
            \overline{\boldsymbol{f}^1} = 0,
        \end{align*}
        \blt $\lambda_1$ est la \motcle{variance} de la PC1:
        \begin{align*}
            \text{Var}(\boldsymbol{f}^1) = \lambda_1.
        \end{align*}
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Sélection des vecteurs $\boldsymbol{v}$}

\begin{itemize}
    \fleche Dans un \motcle{second temps}, on cherche un axe $\boldsymbol{\Delta}_2$ défini par un vecteur $\boldsymbol{v}_2\in\mathbb{R}^p$ tel que
     \begin{align*}
        \boldsymbol{v}_2 &= \argmax{||\boldsymbol{v}|| = 1; \boldsymbol{v} \perp \boldsymbol{v}_1} \text{Var}(\boldsymbol{Z}\boldsymbol{v})\\
        &= \argmax{||\boldsymbol{v}|| = 1; \boldsymbol{v} \perp \boldsymbol{v}_1} \boldsymbol{v}^\top \boldsymbol{R}\boldsymbol{v}.
    \end{align*}
    \fleche On cherche donc l'axe $\boldsymbol{\Delta}_2$ \motcle{orthogonal} au premier axe principal ($\boldsymbol{\Delta}_1$) qui \motcle{maximise la variance des observations projetées} sur ce même axe.
    \fleche On peut montrer que:
        \begin{itemize}
        \blt $\boldsymbol{v}_2$ est le \motcle{vecteur propre} associé à la \motcle{deuxième plus grande} valeur propre $\lambda_2$ de $\boldsymbol{R}$,
        \blt La \motcle{deuxième composante principale} (PC2) $\boldsymbol{f}^2 = \boldsymbol{Z}\boldsymbol{v}_2$ est centrée:
        \begin{align*}
            \overline{\boldsymbol{f}^2} = 0,
        \end{align*}
        \blt $\lambda_2$ est la \motcle{variance} de la PC2:
        \begin{align*}
            \text{Var}(\boldsymbol{f}^2) = \lambda_2.
        \end{align*}
        \fleche Les composantes principales $\boldsymbol{f}^1$ et $\boldsymbol{f}^2$ ne sont pas corrélées.
    \end{itemize}
\end{itemize}

En continuant, on peut obtenir $q \le r$ (r est le rang de $\boldsymbol{Z}$) axes orthogonaux $\boldsymbol{\Delta}_1, \dots, \boldsymbol{\Delta}_q$ sur lesquels les observations sont projetées orthogonalement.
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{En résumé}

\begin{itemize}
    \item[1.] La \motcle{décomposition spectrale} de la matrice $\boldsymbol{R}$ est effectuée et $q \le r$ est choisi.
    \item[2.] La matrice $\boldsymbol{F} = \boldsymbol{Z}\boldsymbol{V}\in \mathbb{R}^{n \times q}$ des \motclef{$q$ composantes principales} est obtenue, où $\boldsymbol{V}$ est la matrice des $q$ premiers vecteurs propres de $\boldsymbol{R}.$
    \begin{itemize}
        \item[-] Les composantes principales $\boldsymbol{f}^\alpha = \boldsymbol{Z}\boldsymbol{v}_\alpha, \alpha = 1, \dots, q$ (colonnes de $\boldsymbol{F}$) sont centrées et de variance $\lambda_\alpha$.
        \item[-] Les éléments $f_{i\alpha}$ sont appelés les \og{}factor coordinates\fg{} ou les \og{}scores\fg{} des observations sur les composantes principales.
    \end{itemize}
\end{itemize}


\begin{table}
    \centering
    $\boldsymbol{F} = $
    \begin{tabular}{c|ccccc|}
        & 1 & \cdots & $\alpha$ & \cdots & $q$\\
        \midrule
        1 &  &  &  &  & \\
        \vdots &  &  & \vdots &  & \\
        $i$ &  & \cdots & $f_{i\alpha}$ & \cdots & \\
        \vdots &  &  & \vdots &  & \\
        $n$ &  &  &  &  & \\
        \midrule
        \midrule
        moyenne &  & \cdots & $0$ & \cdots &\\
        variance &  & \cdots & $\lambda_\alpha$ & \cdots &
    \end{tabular}
\end{table}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Exemple}

\small
\textbf{Exemple:} matrice $\boldsymbol{F}$ des $q = 2$ premières composantes de l'extrait du jeu de données \texttt{eurojob}.

\begin{code}
> acp$ind$coord[ , 1:2]
                Dim.1       Dim.2
Belgium     0.8804620 -0.72493205
Denmark    -1.5141449 -1.50391377
France      0.1719596 -0.05132316
W. Germany  2.3694452  0.38961453
Ireland    -1.4359290  1.72305846
Italy      -0.4717928  0.16749598
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.37]{projection_plan.png}
\end{figure}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Analyse du nuage des variables}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Analyse du nuage des variables}

\small
\begin{itemize}
    \fleche Les 3 variables centrées-réduites forment un nuage dans $\mathbb{R}^6$, et plus spécifiquement, dans l'\motcle{hypersphère de rayon 1}.
\end{itemize}

\begin{code}
> round(t(z), 4)
    Belgium Denmark  France W. Germany Ireland   Italy
Agr -1.2671 -0.3573 -0.1105    -0.7428  1.8017  0.6759
Min  0.3140 -1.8391  0.0449     1.3905  0.5831 -0.4934
Man  0.1560 -1.0240  0.1356     1.8242 -1.2478  0.1560
\end{code}

    
\begin{exampleblock}{Objectif}
    \begin{itemize}
        \fleche Trouver un espace (souvent un plan) tel que les angles entre les variables (c'est-à-dire les corrélations) soient les mieux préservées.
    \end{itemize}
\end{exampleblock}


\begin{columns}[T]
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \centering
            \includegraphics[scale = 0.3]{projection_variables_plan.png}
        \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \centering
            \includegraphics[scale = 0.22]{sphere_unite.png}
        \end{figure}
    \end{column}
\end{columns}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Projection d'une variable sur un axe}

\begin{itemize}
    \fleche La \motcle{coordonnée de la projection} $\boldsymbol{N}$-orthogonale d'un point $\boldsymbol{z}^j\in\mathbb{R}^n$ sur un axe $G_\alpha$ défini par le vecteur unitaire $\boldsymbol{u}_\alpha$ ($\boldsymbol{u}_\alpha^\top \boldsymbol{N} \boldsymbol{u}_\alpha = 1$) est:
    \begin{align*}
        a_{j\alpha} = \langle \boldsymbol{z}^j, \boldsymbol{u}_\alpha \rangle_{\boldsymbol{N}} = (\boldsymbol{z}^j)^\top \boldsymbol{N}\boldsymbol{u}_\alpha,
    \end{align*}
    \fleche Le \motcle{vecteur des coordonnées} sur l'axe $G_\alpha$ pour les $p$ variables est:
    \begin{align*}
        \boldsymbol{a}^\alpha = \begin{pmatrix} a_{1\alpha} \\ \vdots \\ a_{p\alpha} \end{pmatrix} = \boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{u}_\alpha.
    \end{align*}
\end{itemize}

En ACP, les vecteurs $\boldsymbol{u}_1, \boldsymbol{u}_2, \dots$ sont choisis de manière à \motcle{maximiser la somme des cosinus carrés des angles} entre les variables et l'axe de projection.

\begin{alertblock}{Attention: une métrique $\boldsymbol{N}$ dans $\mathbb{R}^n$ est utilisée}
\begin{itemize}
    \fleche Une métrique dans $\mathbb{R}^n$ est une matrice $n \times n$ semi-définie positive.
    \fleche Ici en ACP, $\boldsymbol{N}$ est la matrice diagonale des poids des observations:
    \begin{align*}
        \boldsymbol{N} = \text{diag}(w_1, \dots, w_n).
    \end{align*}
    \fleche Dans le cadre du cours, on ne considère que le cas où les observations ont le même poids $1/n$. On utilise donc la métrique $\boldsymbol{N} = \frac{1}{n}\boldsymbol{I}_n$.
\end{itemize}
\end{alertblock}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Projection d'une variable sur un axe -- Exemple}

\small 
\begin{exemple}
    \begin{itemize}
        \fleche On considère la matrice \texttt{eurojob} centrée-réduite
        \begin{align*}
            \boldsymbol{Z} = 
            \begin{bmatrix}
                -1.2671 & 0.3140 & 0.1560 \\ 
                -0.3573 & -1.8391 & -1.0240 \\ 
                -0.1105 & 0.0449 & 0.1356 \\ 
                -0.7428 & 1.3905 & 1.8242 \\ 
                1.8017 & 0.5831 & -1.2478 \\ 
                0.6759 & -0.4934 & 0.1560 \\ 
            \end{bmatrix}
        \end{align*}
        \fleche On veut projeter les 3 variables (en utilisant la métrique $\boldsymbol{N} = (1/6)\boldsymbol{I}_6$) sur les axes orthogonaux $G_1$ et $G_2$ définis respectivement par les vecteurs unitaires
         \begin{align*}
            \boldsymbol{u}_1 = 
            \begin{bmatrix}
                0.2655155 \\ 
                -0.4566114 \\ 
                0.0518568 \\ 
                0.7145390 \\ 
                -0.4330243 \\ 
                -0.1422757 \\ 
            \end{bmatrix}
            \quad \text{et}\quad
            \boldsymbol{u}_2 = 
            \begin{bmatrix}
                -0.2974757 \\ 
                -0.6171307 \\ 
                -0.0210604 \\ 
                0.1598782 \\ 
                0.7070567 \\ 
                0.0687319 \\ 
            \end{bmatrix}
        \end{align*}
        \fleche Calculer $\boldsymbol{a}^1$ et $\boldsymbol{a}^2$.
    \end{itemize}
\end{exemple}

\textbf{Réponse:}

\begin{itemize}
    \fleche $\boldsymbol{a}^1 = \boldsymbol{Z}^\top\boldsymbol{N}\boldsymbol{u}_1$ nous donne les coordonnées des 3 variables projetées sur l'axe définit par $\boldsymbol{u}_1$.
    \fleche $\boldsymbol{a}^2 = \boldsymbol{Z}^\top\boldsymbol{N}\boldsymbol{u}_2$ nous donne les coordonnées des 3 variables projetées sur l'axe définit par $\boldsymbol{u}_2$.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Sélection des vecteurs $\boldsymbol{u}$}

\begin{itemize}
    \fleche Dans un \motcle{premier temps}, on cherche un axe $G_1$ défini par un vecteur unitaire $\boldsymbol{u}_1\in\mathbb{R}^n$ tel que
    \begin{align*}
        \boldsymbol{u}_1 &= \argmax{||\boldsymbol{u}||_{\boldsymbol{N}} = 1} \sum_{j = 1}^p \cos^2(\theta(\boldsymbol{z}^j, \boldsymbol{u})) \\
        &= \argmax{||\boldsymbol{u}||_{\boldsymbol{N}} = 1} ||\boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{u}||^2.
    \end{align*}
    \fleche On chercher donc l'axe $G_1$ qui \motcle{maximise la somme des cosinus carrés des angles} entre les variables et l'axe de projection.
\end{itemize}

\begin{itemize}
    \fleche On peut montrer qu'avec $\boldsymbol{N} = \frac{1}{n}\boldsymbol{I}_n$:
    \begin{itemize}
        \blt $\boldsymbol{u}_1$ est le \motcle{vecteur propre} associé à la \motcle{plus grande} valeur propre $\lambda_1$ de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$,
        \blt La plus grande valeur propre de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$ est aussi la plus grande valeur propre $\lambda_1$ de $\boldsymbol{R} = \frac{1}{n}\boldsymbol{Z}^\top\boldsymbol{Z}$,
        \blt $\lambda_1$ est la \motcle{somme des cosinus carrés} entre les variables et $\boldsymbol{u}_1$:
        \begin{align*}
            \lambda_1 = \sum_{j = 1}^p \cos^2(\theta(\boldsymbol{z}^j, \boldsymbol{u}_1)).
        \end{align*}
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Sélection des vecteurs $\boldsymbol{u}$}

\begin{itemize}
    \fleche Dans un \motcle{second temps}, on cherche un axe $G_2$ défini par un vecteur unitaire $\boldsymbol{u}_2\in\mathbb{R}^n$ tel que
    \begin{align*}
        \boldsymbol{u}_2 &= \argmax{||\boldsymbol{u}||_{\boldsymbol{N}} = 1; \boldsymbol{u}_2 \perp_{\boldsymbol{N}}\boldsymbol{u}_1} \sum_{j = 1}^p \cos^2(\theta(\boldsymbol{z}^j, \boldsymbol{u})) \\
        &= \argmax{||\boldsymbol{u}||_{\boldsymbol{N}; \boldsymbol{u}_2 \perp_{\boldsymbol{N}}\boldsymbol{u}_1} = 1} ||\boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{u}||^2.
    \end{align*}
    \fleche On chercher donc l'axe $G_2$ \motcle{orthogonal à l'axe $G_1$} qui \motcle{maximise la somme des cosinus carrés des angles} entre les variables et l'axe de projection.
\end{itemize}

\begin{itemize}
    \fleche On peut montrer qu'avec $\boldsymbol{N} = \frac{1}{n}\boldsymbol{I}_n$:
    \begin{itemize}
        \blt $\boldsymbol{u}_2$ est le \motcle{vecteur propre} associé à la \motcle{deuxième plus grande} valeur propre $\lambda_2$ de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$,
        \blt La deuxième plus grande valeur propre de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$ est aussi la deuxième plus grande valeur propre $\lambda_2$ de $\boldsymbol{R} = \frac{1}{n}\boldsymbol{Z}^\top\boldsymbol{Z}$,
        \blt $\lambda_2$ est la \motcle{somme des cosinus carrés} entre les variables et $\boldsymbol{u}_2$:
        \begin{align*}
            \lambda_2 = \sum_{j = 1}^p \cos^2(\theta(\boldsymbol{z}^j, \boldsymbol{u}_2)).
        \end{align*}
    \end{itemize}
\end{itemize}

En continuant, on peut obtenir $q \le r$ (r est le rang de $\boldsymbol{Z}$) axes orthogonaux $G_1, \dots, G_q$ sur lesquels les variables sont projetées orthogonalement.

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{En résumé}

\begin{itemize}
    \item[1.] La \motcle{décomposition spectrale} de la matrice $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$ est effectuée et $q \le r$ est choisi.
    \item[2.] La matrice $\boldsymbol{A} = \boldsymbol{Z}^\top\boldsymbol{N}\boldsymbol{U}\in \mathbb{R}^{p \times q}$ des \motclef{$q$ vecteurs de \og{}loadings\fg{}} est obtenue, où $\boldsymbol{U}$ est la matrice des $q$ premiers vecteurs propres de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$.
    \begin{itemize}
        \item[-] Le vecteur de loading $\boldsymbol{a}^\alpha = \boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{u}_\alpha$ (colonne de $\boldsymbol{A}$) contient les coordonnées des projections des $p$ variables sur l'axe $G_\alpha$.
        \item[-] Les éléments $a_{i\alpha}$ sont appelés les \og{}factor coordinates\fg{} ou les \og{}loadings\fg{} des variables.
    \end{itemize}
\end{itemize}


\begin{table}
    \centering
    $\boldsymbol{A} = $
    \begin{tabular}{c|ccccc|}
        & 1 & \cdots & $\alpha$ & \cdots & $q$\\
        \midrule
        1 &  &  &  &  & \\
        \vdots &  &  & \vdots &  & \\
        $j$ &  & \cdots & $a_{i\alpha}$ & \cdots & \\
        \vdots &  &  & \vdots &  & \\
        $p$ &  &  &  &  & \\
        \midrule
        \midrule
        norme &  & \cdots & $\sqrt{\lambda_\alpha}$ & \cdots &
    \end{tabular}
\end{table}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Exemple}

\small
\textbf{Exemple:} matrice $\boldsymbol{A}$ des $q = 2$ premiers vecteurs de loadings du jeu de données \texttt{eurojob}.

\begin{code}
> acp$var$coord[ , 1:2]
         Dim.1       Dim.2
Agr -0.6475299 0.735384862
Min  0.7090202 0.670068254
Man  0.9543009 0.001144025
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.37]{projection_variables_plan.png}
\end{figure}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Relations de transition}

\begin{itemize}
    \fleche On peut montrer que:
    \begin{itemize}
        \blt on peut obtenir les \motcle{composantes principales} directement par la décomposition spectrale de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$:
        \begin{align*}
            \boldsymbol{f}^\alpha = \boldsymbol{Z}\boldsymbol{v}_\alpha = \sqrt{\lambda_\alpha}\boldsymbol{u}_\alpha,
        \end{align*}
        \blt on peut obtenir les \motcle{vecteurs de loadings} directement par la décomposition spectrale de $\frac{1}{n}\boldsymbol{Z}^\top\boldsymbol{Z}$:
        \begin{align*}
             \boldsymbol{a}^\alpha = \boldsymbol{Z}^\top \boldsymbol{N}\boldsymbol{u}_\alpha = \sqrt{\lambda_\alpha}\boldsymbol{v}_\alpha.
        \end{align*}
        \fleche On a alors que 
        \begin{align*}
            \boldsymbol{F} &= \boldsymbol{U}\boldsymbol{\Lambda}\\
            \boldsymbol{A} &= \boldsymbol{V}\boldsymbol{\Lambda},
        \end{align*}
        où $\boldsymbol{\Lambda} = \text{diag}(\sqrt{\lambda_1}, \dots, \sqrt{\lambda_q})$.
    \end{itemize}
    \fleche En pratique, on va donc réaliser l'ACP sur un des 2 nuages (souvent le nuage des observations).
    \fleche On va ensuite dériver les résultats pour le nuage des variables sans avoir à faire la décomposition spectrale des 2 matrices.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Relations de transition}

\begin{itemize}
    \fleche Cela signifie que:
    \begin{itemize}
        \blt les vecteurs propres $\boldsymbol{u}_\alpha$ de $\frac{1}{n}\boldsymbol{Z}\boldsymbol{Z}^\top$ sont les \motcle{composantes principales standardisées}:
        \begin{align*}
            \boldsymbol{u}_\alpha = \frac{\boldsymbol{f}^\alpha}{\sqrt{\lambda_\alpha}},
        \end{align*}
        \blt les \motcle{loadings} sont les \motcle{corrélations entre les variables et les composantes principales}:
        \begin{align*}
            a_{j\alpha} = \text{Cor}(\boldsymbol{x}^j, \boldsymbol{f}^\alpha) = \sqrt{\lambda_\alpha} v_{j\alpha}.
        \end{align*}
    \end{itemize}
\end{itemize}

Cette dernière propriété est importante, car elle permet de connecter la représentation des observations avec la représentation des variables. Autrement dit, elle permet de faire la représentation des observations et des variables sur le même plan. Un tel graphique est appelé \og{}biplot\fg{}.
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Interprétation des résultats}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Variance des composantes principales}
    

\begin{itemize}
    \fleche Les composantes principales (colonnes de $\boldsymbol{F}$) sont \motclef{$q$ nouvelles variables non corrélées} et de \motcle{variance maximale}, avec
    \begin{align*}
        \text{Var}(\boldsymbol{f}^\alpha) = \lambda_\alpha.
    \end{align*}
    \fleche Il en découle que l'\motcle{inertie} du nuage des observations projeté sur les $q$ premières dimensions est:
    \begin{align*}
        \boldsymbol{I}(\boldsymbol{F}) = \lambda_1 + \dots + \lambda_q.
    \end{align*}
\end{itemize}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche Les $p =3$ valeurs propres de la matrice de corrélation $\boldsymbol{R}$ sont
    \begin{code}
> acp[["eig"]][ , 1]
   comp 1    comp 2    comp 3 
1.8326949 0.9897837 0.1775214 
    \end{code}
    \fleche Donc
    \begin{align*}
        \text{Var}(\boldsymbol{f}^1) &= 1.8327\\
        \text{Var}(\boldsymbol{f}^2) &= 0.9898
    \end{align*}
    \fleche Et l'inertie des 6 observations (pays) projetées sur les $q = 2$ premières dimensions est:
    \begin{align*}
        \lambda_1 + \lambda_2 = 1.8327 + 0.9898 = 2.8225.
    \end{align*}
\end{itemize}


\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Inertie totale}

\begin{itemize}
    \fleche En ACP standardisée, l'\motcle{inertie totale} du nuage de points est la somme des variances des colonnes de $\boldsymbol{Z}$:
    \begin{align*}
        \boldsymbol{I}(\boldsymbol{Z}) = \sum_{j = 1}^n \text{Var}(\boldsymbol{z}^j) = p.
    \end{align*}
    \fleche Lorsque $q = r$ (on choisit autant de composantes principales que le rang de $\boldsymbol{Z}$), l'\motcle{inertie totale} est égale à la somme des variances de toutes les composantes principales:
    \begin{align*}
        \boldsymbol{I}(\boldsymbol{F}) = \lambda_1 + \dots + \lambda_r = \boldsymbol{I}(\boldsymbol{Z}) = p.
    \end{align*}
\end{itemize}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche L'inertie des 6 observations projetées sur les $q = 3$ axes principaux est:
    \begin{align*}
        \boldsymbol{I}(\boldsymbol{F}) = \lambda_1 + \lambda_2 + \lambda_3 = 1.8327 + 0.9898 + 0.1775 = 3.
    \end{align*}
\end{itemize}


\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Qualité de la représentation}

\small
\begin{itemize}
    \fleche La \motcle{proportion de l'inertie des données expliquée} par la \motcle{$\alpha^\text{e}$ composante principale} est
    \begin{align*}
        \frac{\textbf{Var}(\boldsymbol{f}^\alpha)}{\boldsymbol{I}(\boldsymbol{Z})} = \frac{\lambda_\alpha}{\lambda_1 + \dots + \lambda_r}.
    \end{align*}
    \fleche La \motcle{proportion de l'inertie des données expliquée} par les \motcle{$q$ premières composantes principales} est
    \begin{align*}
        \frac{\boldsymbol{I}(\boldsymbol{F})}{\boldsymbol{I}(\boldsymbol{Z})} = \frac{\lambda_1 + \dots + \lambda_q}{\lambda_1 + \dots + \lambda_r}.
    \end{align*}
\end{itemize}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche Quelle est la proportion de l'inertie expliquée par la première composante principale?
    \begin{align*}
        \frac{1.8327}{3} = 61.09\%.
    \end{align*}
    \fleche Quelle est la qualité de la représentation des données sur le plan principal?
    \begin{align*}
        \frac{1.8327 + 0.9898}{3} = 94.08\%.
    \end{align*}
\end{itemize}

\begin{code}
> acp[["eig"]]
       eigenvalue percentage of variance cumulative percentage of variance
comp 1  1.8326949               61.08983                          61.08983
comp 2  0.9897837               32.99279                          94.08262
comp 3  0.1775214                5.91738                         100.00000
\end{code}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Qualité de la représentation}

    \begin{itemize}
        \fleche La librairie \texttt{factoextra} permet de visualiser les résultats de la fonction \texttt{FactoMineR::PCA}.
        \fleche On peut ainsi visualiser les valeurs propres avec la commande
        \small
        \begin{code}
library(ggplot2)
library(factoextra)

fviz_eig(acp)
        \end{code}
    \begin{figure}
        \centering
        \includegraphics[scale = 0.3]{scree_plot.png}
    \end{figure}
    \end{itemize}

\end{frame}


% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Qualité de la représentation des observations}

\footnotesize
\begin{itemize}
    \fleche Si 2 observations sont \motcle{bien projetées}, alors leur distance dans le \motcle{plan de projection} est similaire à leur distance dans $\mathbb{R}^p$.
    \fleche La \motcle{qualité de la projection de l'observation $i$} sur l'axe $\Delta_\alpha$ est mesurée par le carré du cosinus de l'angle $\theta_{i\alpha}$ formé entre le vecteur $\boldsymbol{z}_i$ et l'axe $\Delta_\alpha$:
    \begin{align*}
        \cos^2(\theta_{i\alpha}) = \frac{f_{i\alpha}^2}{||\boldsymbol{z}_i||^2}
    \end{align*}
    \fleche La \motcle{qualité de la projection de l'observation $i$} sur le \motcle{plan} ($\Delta_\alpha, \Delta_{\alpha'}$) est mesurée par le carré du cosinus de l'angle $\theta_{i(\alpha, \alpha')}$ entre le point $\boldsymbol{z}_i$ et le plan ($\Delta_\alpha, \Delta_{\alpha'}$):
    \begin{align*}
        \cos^2(\theta_{i(\alpha, \alpha')}) = \frac{f_{i\alpha}^2 + f_{i\alpha'}^2}{||\boldsymbol{z}_i||^2}
    \end{align*}
    \fleche Plus cette valeur est près de 1, meilleure est la projection.
\end{itemize}

\begin{alertblock}{Attention!}
\begin{itemize}
    \fleche Seules les observations bien projetées peuvent être interprétées.
\end{itemize}
\end{alertblock}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche En \texttt{R}, on peut récupérer la qualité de la projection des observations avec la commande suivante:
    \scriptsize
    \begin{code}
> acp$ind$cos2
               Dim.1      Dim.2        Dim.3
Belgium    0.4485097 0.30405015 0.2474401363
Denmark    0.5029432 0.49616937 0.0008873911
France     0.9064981 0.08074967 0.0127522800
W. Germany 0.9658347 0.02611438 0.0080508690
Ireland    0.4009155 0.57728058 0.0218038933
Italy      0.3071478 0.03871271 0.6541395224
    \end{code}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Qualité de la représentation des observations}

\small
\begin{itemize}
    \fleche On peut visualiser la qualité de la projection des observations avec la fonction \texttt{factoextra::fviz\_cos2}.
    \fleche Par exemple, sur le plan principal:
    \begin{code}
> fviz_cos2(acp, "ind", axes = c(1, 2))
    \end{code}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale = 0.4]{qualite_repr_ind_plan.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------
\begin{frame}[fragile]{Contribution des observations}

\begin{itemize}
    \fleche Les observations qui apportent une \motcle{contribution importante} à l'inertie des données projetées sont source d'\motcle{instabilité}.  
    \fleche L'inertie (la variance) sur l'axe $\boldsymbol{\Delta}_\alpha$ est $\lambda_\alpha = \sum_{i =1}^n w_if_{i\alpha}^2$, avec habituellement $w_i = 1/n \quad \forall i$.
    \fleche La \motcle{contribution relative} d'une observation $i$ à l'inertie de l'axe $\Delta_\alpha$ est
    \begin{align*}
        \text{Ctr}(i, \alpha) = \frac{w_i f_{i\alpha}^2}{\lambda_\alpha}.
    \end{align*}
    \fleche La \motcle{contribution relative} d'une observation $i$ à l'inertie du plan $(\Delta_\alpha, \Delta_{\alpha'})$ est
    \begin{align*}
        \text{Ctr}(i, (\alpha, \alpha')) = \frac{w_i f_{i\alpha}^2 + w_i f_{i\alpha'}^2}{\lambda_\alpha + \lambda_{\alpha'}}.
    \end{align*}
\end{itemize}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche En \texttt{R}, on peut récupérer la contribution des observations à l'inertie des axes principaux avec la commande suivante:
    \footnotesize
    \begin{code}
> acp$ind$contrib
                Dim.1       Dim.2       Dim.3
Belgium     7.0498489  8.84918077 40.15295712
Denmark    20.8493959 38.08503309  0.37977730
France      0.2689128  0.04435425  0.03905457
W. Germany 51.0566019  2.55610544  4.39370883
Ireland    18.7510043 49.99291844 10.52798203
Italy       2.0242362  0.47240801 44.50652015
    \end{code}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Contribution des observations}

\small
\begin{itemize}
    \fleche On peut visualiser la qualité de la contribution des observations à l'inertie des axes principaux avec la fonction \texttt{factoextra::fviz\_contrib}.
    \fleche Par exemple, sur le plan principal:
    \begin{code}
> fviz_contrib(acp, "ind", axes = c(1, 2))
    \end{code}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale = 0.4]{contrib_ind_plan.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Interprétation du cercle des corrélations}

Si 2 variables sont \motcle{bien projetées}, alors leur \motcle{angle dans le plan de projection} est similaire à celui dans $\mathbb{R}^n$. La corrélation entre ces 2 variables est donc similaire au cosinus de leur angle dans le plan de projection. 

\begin{itemize}
    \fleche La \motcle{qualité de la projection d'une variable $j$ sur l'axe $G_\alpha$} est mesurée par le carré du cosinus de l'angle $\theta_{j\alpha}$ entre le point $\boldsymbol{z}^j$ et l'axe $G_\alpha$:
    \begin{align*}
        \cos^2(\theta_{j\alpha}) = \frac{a_{j\alpha}^2}{||\boldsymbol{z}^j||^2_{\boldsymbol{N}}} = a_{j\alpha}^2.
    \end{align*}
    \fleche La \motcle{qualité de la projection d'une variable $j$ sur le plan ($G_\alpha, G_{\alpha'}$)} est mesurée par le carré du cosinus de l'angle $\theta_{j(\alpha, \alpha')}$ entre le point $\boldsymbol{z}^j$ et le plan ($G_\alpha, G_{\alpha'}$):
    \begin{align*}
        \cos^2(\theta_{j(\alpha\alpha')}) = a_{j\alpha}^2 + a_{j\alpha'}^2.
    \end{align*}
    $\sqrt{\cos^2(\theta_{j(\alpha\alpha')})}$ est alors la \og{}longeur de la flèche\fg{} sur le plan de projection.
\end{itemize}

Plus la flèche se rapproche du cercle unitaire, plus la qualité de la projection de la variable est élevée.

\begin{alertblock}{Attention!}
\begin{itemize}
    \fleche Seules les variables bien projetées peuvent être interprétées.
\end{itemize}
\end{alertblock}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation du cercle des corrélations}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche En \texttt{R}, on peut récupérer la qualité de la projection des variables sur les axes principaux avec la commande suivante:
    \small
    \begin{code}
> acp$var$cos2
        Dim.1        Dim.2      Dim.3
Agr 0.4192950 5.407909e-01 0.03991412
Min 0.5027097 4.489915e-01 0.04829884
Man 0.9106902 1.308793e-06 0.08930845
    \end{code}
    \normalsize
    \fleche Le $\cos^2$ de la variable \texttt{Agr} sur l'axe $G_1$ est $0.4193$.
    \fleche Le $\cos^2$ de la variable \texttt{Agr} sur le plan $(G_1, G_2)$ est $0.4193 + 0.5408 = 0.9601$.
    \fleche La variable \texttt{Agr} est donc très bien projetée sur le plan, et la longueur de la flèche sur le cercle des corrélations sera $\sqrt{0.9601} = 0.9798$.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation du cercle des corrélations}

\begin{code}
fviz_pca_var(acp, col.var = "cos2")
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.46]{graph_vars_cos2.png}
\end{figure}

\begin{itemize}
    \fleche Les variables sont-elles globalement bien projetées sur ce plan?
    \fleche Interpréter le cercle des corrélations.
\end{itemize}

\end{frame}


% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Contribution des variables}

Les variables ayant une \motcle{contribution importante} à l'intertie des données projetées sont utiles pour interpréter les axes principaux. Pour déterminer la contribution d'une variable à un axe (ou un plan), on évalue la proportion de l'inertie totale de l'axe (ou du plan) expliquée par la variable. 

\begin{itemize}
    \fleche L'inertie totale de l'axe $\Delta_\alpha$ est $\lambda_\alpha = \sum_{j=1}^p a_{j\alpha}^2 = \sum_{j=1}^p \cos^2(\theta_{j\alpha})$.
    \fleche La contribution relative d'une variable $j$ à l'inertie de l'axe $\Delta_\alpha$ est
    \begin{align*}
        \text{Ctr}(j, \alpha) = \frac{a_{j\alpha}^2}{\lambda_\alpha}.
    \end{align*}
    \fleche La contribution relative d'une variable $j$ à l'inertie du plan $(\Delta_\alpha, \Delta_{\alpha'})$ est
    \begin{align*}
        \text{Ctr}(j, (\alpha, \alpha')) = \frac{a_{j\alpha}^2 + a_{j\alpha'}^2}{\lambda_\alpha + \lambda_{\alpha'}}.
    \end{align*}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Contribution des variables}

\textbf{Exemple:} extrait d'\texttt{eurojob}

\begin{itemize}
    \fleche En \texttt{R}, on peut récupérer la contribution relative des variables à l'inertie des axes principaux (en \%) avec la commande suivante:
    \small
    \begin{code}
> acp$var$contrib
       Dim.1        Dim.2    Dim.3
Agr 22.87860 5.463728e+01 22.48412
Min 27.43008 4.536259e+01 27.20733
Man 49.69132 1.322302e-04 50.30855
    \end{code}
    \fleche On peut également visualiser ces contributions sur un graphique:
    \begin{code}
> fviz_contrib(acp, "var", axes = 1)
    \end{code}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{contrib_var_dim1.png}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation globale (avec un biplot)}

\small
\begin{code}
fviz_pca_biplot(acp, pointsize = "cos2", col.var = "cos2")
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{biplot_countries.png}
\end{figure}

\begin{itemize}
    \fleche \motcle{Italy} mal projetée: on ne peut pas vraiment l'interpréter sur ce plan.
    \fleche \motcle{Denmark} a un secteur minier peu développé.
    \fleche \motcle{W. Germany} a les secteurs minier et manufacturier très développés et un secteur agricole peu développé.
    \fleche La \motcle{France} est un pays qui se situe dans la moyenne dans les 3 secteurs.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\section{Application sur un jeu de données}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Jeu de données \og{}températures de France\fg{}}

\footnotesize
\begin{code}
library(tidyverse)
france <- read.table("http://freakonometrics.free.fr/FR_temp.txt", header = TRUE, dec = ",")
\end{code}
\normalsize
    
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|cccccccccccccccc}
  \toprule
 & Jan & Fev & Mar & Avr & Mai & Juin & Juil & Aout & Sep & Oct & Nov & Dec & Lati & Long & Moye & Ampl \\ 
  \hline
Bordeaux & 5.60 & 6.60 & 10.30 & 12.80 & 15.80 & 19.30 & 20.90 & 21.00 & 18.60 & 13.80 & 9.10 & 6.20 & 44.50 & -0.34 & 13.33 & 15.40 \\ 
  Brest & 6.10 & 5.80 & 7.80 & 9.20 & 11.60 & 14.40 & 15.60 & 16.00 & 14.70 & 12.00 & 9.00 & 7.00 & 48.24 & -4.29 & 10.77 & 10.20 \\ 
  Clermont & 2.60 & 3.70 & 7.50 & 10.30 & 13.80 & 17.30 & 19.40 & 19.10 & 16.20 & 11.20 & 6.60 & 3.60 & 45.47 & 3.05 & 10.94 & 16.80 \\ 
  Grenoble & 1.50 & 3.20 & 7.70 & 10.60 & 14.50 & 17.80 & 20.10 & 19.50 & 16.70 & 11.40 & 6.50 & 2.30 & 45.10 & 5.43 & 10.98 & 18.60 \\ 
  Lille & 2.40 & 2.90 & 6.00 & 8.90 & 12.40 & 15.30 & 17.10 & 17.10 & 14.70 & 10.40 & 6.10 & 3.50 & 50.38 & 3.04 & 9.73 & 14.70 \\ 
  Lyon & 2.10 & 3.30 & 7.70 & 10.90 & 14.90 & 18.50 & 20.70 & 20.10 & 16.90 & 11.40 & 6.70 & 3.10 & 45.45 & 4.51 & 11.36 & 18.60 \\ 
  Marseille & 5.50 & 6.60 & 10.00 & 13.00 & 16.80 & 20.80 & 23.30 & 22.80 & 19.90 & 15.00 & 10.20 & 6.90 & 43.18 & 5.24 & 14.23 & 17.80 \\ 
  Montpellier & 5.60 & 6.70 & 9.90 & 12.80 & 16.20 & 20.10 & 22.70 & 22.30 & 19.30 & 14.60 & 10.00 & 6.50 & 43.36 & 3.53 & 13.89 & 17.10 \\ 
  Nantes & 5.00 & 5.30 & 8.40 & 10.80 & 13.90 & 17.20 & 18.80 & 18.60 & 16.40 & 12.20 & 8.20 & 5.50 & 47.13 & -1.33 & 11.69 & 13.80 \\ 
  Nice & 7.50 & 8.50 & 10.80 & 13.30 & 16.70 & 20.10 & 22.70 & 22.50 & 20.30 & 16.00 & 11.50 & 8.20 & 43.42 & 7.15 & 14.84 & 15.20 \\ 
  Paris & 3.40 & 4.10 & 7.60 & 10.70 & 14.30 & 17.50 & 19.10 & 18.70 & 16.00 & 11.40 & 7.10 & 4.30 & 48.52 & 2.20 & 11.18 & 15.70 \\ 
  Rennes & 4.80 & 5.30 & 7.90 & 10.10 & 13.10 & 16.20 & 17.90 & 17.80 & 15.70 & 11.60 & 7.80 & 5.40 & 48.05 & -1.41 & 11.13 & 13.10 \\ 
  Strasbourg & 0.40 & 1.50 & 5.60 & 9.80 & 14.00 & 17.20 & 19.00 & 18.30 & 15.10 & 9.50 & 4.90 & 1.30 & 48.35 & 7.45 & 9.72 & 18.60 \\ 
  Toulouse & 4.70 & 5.60 & 9.20 & 11.60 & 14.90 & 18.70 & 20.90 & 20.90 & 18.30 & 13.30 & 8.60 & 5.50 & 43.36 & 1.26 & 12.68 & 16.20 \\ 
  Vichy & 2.40 & 3.40 & 7.10 & 9.90 & 13.60 & 17.10 & 19.30 & 18.80 & 16.00 & 11.00 & 6.60 & 3.40 & 46.08 & 3.26 & 10.72 & 16.90 \\ 
   \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\begin{itemize}
    \fleche Jeu de données qui nous renseigne sur la \motcle{température moyenne} pour chaque mois de l'année pour plusieurs villes de France.
    \fleche On a aussi la position géographique de chaque ville (\motcle{latitude} et \motcle{longitude}) ainsi que la \motcle{température moyenne annuelle} et l'\motcle{amplitude thermique annuelle} (différence entre mois le plus chaud et mois le plus froid).
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------


\begin{frame}[fragile]{Jeu de données \og{}températures de France\fg{} centré-réduit}

\small
\begin{itemize}
    \fleche On va faire une ACP sur le jeu de données centré-réduit, en utilisant seulement les 12 variables des températures moyennes pour chaque mois de l'année.
    \footnotesize
    \begin{code}
centrer_reduire <- function(colonne) {
  moyenne <- mean(colonne)
  ecart_type <- sqrt(mean((colonne - moyenne) ^ 2))
  
  (colonne - moyenne) / ecart_type
}

france_norm <- 
  france %>% 
  select(Janv:Dece) %>% 
  mutate_all(centrer_reduire)
    \end{code}
\end{itemize}

\vspace{-0.4cm}
\scriptsize
\begin{table}
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l|cccccccccccc}
  \toprule
 & Jan & Fev & Mar & Avr & Mai & Juin & Juil & Aout & Sep & Oct & Nov & Dec \\ 
  \hline
Bordeaux & 0.84 & 0.98 & 1.40 & 1.33 & 0.94 & 0.85 & 0.52 & 0.74 & 0.90 & 0.84 & 0.67 & 0.72 \\ 
  Brest & 1.10 & 0.54 & -0.29 & -1.30 & -1.95 & -1.98 & \colorbox{gray!40}{-2.06} & -1.83 & -1.28 & -0.18 & 0.62 & 1.14 \\ 
  Clermont & -0.71 & -0.63 & -0.50 & -0.50 & -0.44 & -0.31 & -0.21 & -0.24 & -0.44 & -0.63 & -0.76 & -0.66 \\ 
  Grenoble & -1.28 & -0.90 & -0.36 & -0.28 & 0.05 & -0.02 & 0.13 & -0.03 & -0.16 & -0.52 & -0.82 & -1.35 \\ 
  Lille & -0.81 & -1.07 & -1.51 & -1.52 & -1.40 & -1.46 & -1.33 & -1.27 & -1.28 & -1.09 & -1.05 & -0.71 \\ 
  Lyon & -0.97 & -0.85 & -0.36 & -0.06 & 0.32 & 0.38 & 0.42 & 0.27 & -0.05 & -0.52 & -0.70 & -0.92 \\ 
  Marseille & 0.79 & 0.98 & 1.20 & 1.48 & 1.63 & 1.71 & 1.69 & 1.66 & 1.63 & 1.52 & 1.30 & 1.09 \\ 
  Montpellier & 0.84 & 1.03 & 1.13 & 1.33 & 1.22 & 1.31 & 1.39 & 1.41 & 1.30 & 1.29 & 1.19 & 0.87 \\ 
  Nantes & 0.53 & 0.26 & 0.11 & -0.13 & -0.37 & -0.37 & -0.50 & -0.50 & -0.33 & -0.07 & 0.16 & 0.35 \\ 
  Nice & 1.82 & \colorbox{gray!40}{2.03} & 1.74 & 1.70 & 1.56 & 1.31 & 1.39 & 1.51 & 1.86 & \colorbox{gray!40}{2.08} & \colorbox{gray!40}{2.05} & 1.77 \\ 
  Paris & -0.30 & -0.41 & -0.43 & -0.20 & -0.09 & -0.19 & -0.36 & -0.45 & -0.55 & -0.52 & -0.47 & -0.29 \\ 
  Rennes & 0.43 & 0.26 & -0.23 & -0.64 & -0.92 & -0.94 & -0.94 & -0.91 & -0.72 & -0.41 & -0.07 & 0.29 \\ 
  Strasbourg & -1.84 & -1.85 & -1.78 & -0.86 & -0.30 & -0.37 & -0.41 & -0.65 & -1.06 & -1.60 & -1.74 & -1.87 \\ 
  Toulouse & 0.37 & 0.42 & 0.65 & 0.45 & 0.32 & 0.50 & 0.52 & 0.69 & 0.74 & 0.55 & 0.39 & 0.35 \\ 
  Vichy & -0.81 & -0.79 & -0.77 & -0.79 & -0.57 & -0.42 & -0.26 & -0.39 & -0.55 & -0.75 & -0.76 & -0.76 \\ 
   \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Graphe des individus}

\small
\begin{code}
library(FactoMineR)
library(factoextra)
acp <- PCA(france_norm)
fviz_pca_ind(acp)
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.35]{graph_individuals.png}
\end{figure}

\begin{itemize}
    \fleche Excellente qualité de projection (98.82\%), on peut donc \motcle{interpréter sans \og{}danger\fg{}} les distances entre individus.
    \fleche \motcle{Montpellier} et \motcle{Marseille} proches: signifie que les températures moyennes sont semblables, quel que soit le moment de l'année.
    \fleche \motcle{Lille} et \motcle{Nice} opposées sur le premier axe: 2 villes très différentes en termes de températures mensuelles moyennes puisque l'axe 1 est celui qui sépare le mieux les points.
\end{itemize}
    

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}{Interprétation du premier axe principal}
    
\begin{itemize}
    \fleche Qu'est-ce qui oppose \motcle{Lille} à \motcle{Nice}?
    \begin{itemize}
        \item[-] On peut avoir une bonne connaissance des données et dire qu'à Nice il fait plutôt \motcle{chaud}, alors qu'à Lille, il fait plutôt \motcle{froid}.
        \item[-] On peut aussi raisonner uniquement à partir de l'ACP: on va alors regarder les \motcle{corrélations} entre les \motcle{variables originales} et les \motcle{composantes principales} $\boldsymbol{f}^1$ et $\boldsymbol{f}^2$.
    \end{itemize}
    \fleche Par exemple, si $\text{Cor}(\texttt{janvier}, \boldsymbol{f}^1)$ est \motcle{proche de 1}, ça veut dire que la température en janvier est \motcle{liée positivement} à la coordonnée sur l'axe 1. Dans ce cas:
    \begin{itemize}
        \item[-] les villes où il fait \motcle{froid} en janvier ont une \motcle{faible} coordonnée sur l'axe 1 et sont à \motcle{gauche},
        \item[-] les villes où il fait \motcle{chaud} en janvier ont une \motcle{forte} coordonnée sur l'axe 1 et sont à \motcle{droite}.
    \end{itemize}
    \fleche Au contraire, si $\text{Cor}(\texttt{janvier}, \boldsymbol{f}^1)$ est \motcle{proche de -1}, ça veut dire que la température en janvier est \motcle{liée négativement} à la coordonnée sur l'axe 1. Dans ce cas:
    \begin{itemize}
        \item[-] les villes où il fait \motcle{froid} en janvier ont une \motcle{forte} coordonnée sur l'axe 1 et sont à \motcle{droite},
        \item[-] les villes où il fait \motcle{chaud} en janvier ont une \motcle{faible} coordonnée sur l'axe 1 et sont à \motcle{gauche}.
    \end{itemize}
    \fleche On va donc regarder le \motcle{cercle des corrélations}.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation du premier axe principal}

\small
\begin{code}
fviz_pca_var(acp)
\end{code}

\vspace{-0.2cm}
\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{graph_variables.png}
\end{figure}

\begin{itemize}
    \fleche Toutes les variables sont positivement et assez fortement corrélées à l'axe 1 (corrélation avec l'axe 1 = coordonnée sur l'axe 1).
    \fleche Les villes à \motcle{gauche} (coordonnée faible sur l'axe 1) sont donc celles où il fait \motcle{froid} tous les mois de l'année, alors que les villes à \motcle{droite} (coordonnée forte sur l'axe 1) sont celles où il fait \motcle{chaud} tous les mois de l'année.\footnote{Relativement aux autres villes du jeu de données}
    \fleche On peut donc dire que le premier axe \motcle{sépare les villes chaudes des villes froides}. On peut l'interpréter comme la \motcle{moyenne de température annuelle}.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation du deuxième axe principal}

\small
\vspace{-0.2cm}
\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{graph_variables.png}
\end{figure}

\begin{itemize}
    \fleche Les mois d'\motcle{hiver} sont \motcle{positivement corrélés} avec l'axe 2, tandis que les mois d'\motcle{été} sont \motcle{négativement corrélés} avec l'axe 2. 
    \fleche Les villes en \motcle{haut} (coordonnée forte sur l'axe 2) sont donc celles où il fait plutôt \motcle{chaud l'hiver} et plutôt \motcle{froid l'été} (amplitude thermique faible).
    \fleche Les villes en \motcle{bas} (coordonnée faible sur l'axe 2) sont donc celles où il fait plutôt \motcle{froid l'hiver} et plutôt \motcle{chaud l'été} (amplitude thermique élevée).
    \fleche On peut donc interpréter l'axe 2 comme l'\motcle{amplitude thermique}.
\end{itemize}
    
\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Qualité de la projection des individus}

\begin{code}
fviz_pca_ind(acp, col.ind = "cos2")
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.35]{graph_ind_cos2.png}
\end{figure}

\begin{itemize}
    \fleche \motcle{Paris} est la ville la moins bien projetée. 
    \fleche Cependant, toutes les villes ont un \motcle{cosinus carré de l'angle avec le plan principal} de \motcle{plus de 0.9}: on peut donc interpréter sans danger toutes les distances.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Contribution des individus}

\small
\begin{code}
fviz_pca_ind(acp, col.ind = "contrib")
\end{code}

\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{graph_ind_contrib.png}
\end{figure}

\begin{itemize}
    \fleche Les observations avec des coordonnées élevées ont habituellement une plus grande contribution aux axes principaux.
    \fleche Les observations avec de grandes contributions sont source d'instabilité: si on les retire de l'ACP, ça peut changer les résultats de manière significative.
        \begin{itemize}
            \item[-] Si une ou certaines observations ont des contributions très élevées, ça peut être une bonne idée de refaire l'ACP sans celles-ci.
        \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------------------------------------------------------

\begin{frame}[fragile]{Interprétation globale (avec un biplot)}

\small
\begin{code}
fviz_pca_biplot(acp, pointsize = "cos2", col.var = "cos2")
\end{code}    

\begin{figure}
    \centering
    \includegraphics[scale = 0.3]{biplot_villes.png}
\end{figure}



\end{frame}


% ------------------------------------------------------------------------------------------------------------------------------

\end{document}
